{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u-07PRu2HtxG",
        "outputId": "e8f5dd79-b858-46f4-fcba-04a74c5b45da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n"
      ],
      "metadata": {
        "id": "cxWExbiWH75b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ld_Edr4TKKx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "def read_sentences_from_file(file_path, max_sentences=60):\n",
        "    \"\"\"\n",
        "    Read sentences from a file, handling potential encoding issues.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            for line in file:\n",
        "                sentences.append(line.strip())\n",
        "                if len(sentences) >= max_sentences:\n",
        "                    break\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"UnicodeDecodeError: Unable to decode file with utf-8 encoding. Trying with latin-1 encoding.\")\n",
        "        with open(file_path, \"r\", encoding=\"latin-1\") as file:\n",
        "            for line in file:\n",
        "                sentences.append(line.strip())\n",
        "                if len(sentences) >= max_sentences:\n",
        "                    break\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file '{file_path}': {str(e)}\")\n",
        "    return sentences\n",
        "\n",
        "def classify_sentences(sentences):\n",
        "    \"\"\"\n",
        "    Perform zero-shot classification for each sentence.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Perform classification in batches of 10 sentences\n",
        "        batch_size = 10\n",
        "        results = []\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            batch = sentences[i:i + batch_size]\n",
        "            batch_results = classifier(batch, candidate_labels=[\"positive\", \"neutral\", \"negative\"], multi_label=False)\n",
        "            results.extend(batch_results)\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error during classification: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def write_results_to_csv(output_file, sentences, results):\n",
        "    \"\"\"\n",
        "    Write classification results to a CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            csv_writer = csv.writer(csvfile)\n",
        "            csv_writer.writerow(['Sentence', 'Predicted Sentiment', 'Score'])\n",
        "\n",
        "            for sentence, result in zip(sentences, results):\n",
        "                predicted_sentiment = result['labels'][0]\n",
        "                score = result['scores'][0]\n",
        "                csv_writer.writerow([sentence, predicted_sentiment, score])\n",
        "\n",
        "        print(f'CSV file \"{output_file}\" has been created with the classification results.')\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to CSV file '{output_file}': {str(e)}\")\n",
        "\n",
        "# Example usage with a file\n",
        "file_path = \"/content/Sentences_AllAgree.txt\"\n",
        "output_file = 'output.csv'\n",
        "\n",
        "# Read sentences from file\n",
        "sentences = read_sentences_from_file(file_path, max_sentences=60)\n",
        "\n",
        "if sentences:\n",
        "    # Perform zero-shot classification for each sentence\n",
        "    results = classify_sentences(sentences)\n",
        "\n",
        "    if results:\n",
        "        # Write results to CSV\n",
        "        write_results_to_csv(output_file, sentences, results)\n",
        "    else:\n",
        "        print(\"Classification failed. Check the input data.\")\n",
        "else:\n",
        "    print(\"No sentences found in the input file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pVX-3BCjoH-",
        "outputId": "a3c6859e-4ce9-484e-8158-4bd84e5de7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UnicodeDecodeError: Unable to decode file with utf-8 encoding. Trying with latin-1 encoding.\n",
            "CSV file \"output.csv\" has been created with the classification results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Dl5t4y3yvN8o",
        "outputId": "f2c8768f-ac5d-4fa3-86d5-185292282aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.12-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.4/328.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.12\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.7-py3-none-any.whl (983 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.12 (from langchain)\n",
            "  Downloading langchain_core-0.2.12-py3-none-any.whl (355 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.8/355.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.84-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.12->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.7 langchain-core-0.2.12 langchain-text-splitters-0.2.2 langsmith-0.1.84 orjson-3.10.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('openai-key')"
      ],
      "metadata": {
        "id": "YaasHUQzwfy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "0xloCnIJxhNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.First do 50 to 60 (equally distributed between neutral, positive, and negative), zero-shot\n",
        "prompting to determine accuracy either using an API (chat completion) or a chatbot.\n",
        "Get a baseline accuracy."
      ],
      "metadata": {
        "id": "U6upWOhpXBhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import random\n",
        "\n",
        "# Step 1: Read Sentences from File and Categorize Sentiments\n",
        "def read_sentences_and_categorize(file_path):\n",
        "    neutral_sentences = []\n",
        "    positive_sentences = []\n",
        "    negative_sentences = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:\n",
        "        sentences = file.readlines()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentiment = categorize_sentiment(sentence)\n",
        "        if sentiment == 0:\n",
        "            neutral_sentences.append(sentence.strip())\n",
        "        elif sentiment == 1:\n",
        "            positive_sentences.append(sentence.strip())\n",
        "        elif sentiment == -1:\n",
        "            negative_sentences.append(sentence.strip())\n",
        "\n",
        "    return neutral_sentences, positive_sentences, negative_sentences\n",
        "\n",
        "# Placeholder function for sentiment categorization (replace with actual logic)\n",
        "def categorize_sentiment(sentence):\n",
        "    # Replace with your actual sentiment analysis logic\n",
        "    if \"good\" in sentence.lower() or \"positive keyword\" in sentence.lower():\n",
        "        return 1  # Positive sentiment\n",
        "    elif \"bad\" in sentence.lower() or \"negative keyword\" in sentence.lower():\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "# Step 2: Fine-tune the Model with Selected Sentences (First 60 Sentences)\n",
        "def fine_tune_model(sentences):\n",
        "    fine_tuned_responses = []\n",
        "\n",
        "    for sentence in sentences[:60]:  # Take only the first 60 sentences\n",
        "        prompt = f\"Input: {sentence}\\nOutput:\\n\"\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Fine-tuning chatbot with input-output examples\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        #fine_tuned_responses.append(response.choices[0].message['content'] # Access 'message' dictionary\n",
        "\n",
        "    return  response.choices[0].message.content\n",
        "\n",
        "# Step 3: Evaluate Baseline Accuracy\n",
        "def evaluate_baseline_accuracy(sentences):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for sentiment_type, sentences_list in sentences.items():\n",
        "        for sentence in sentences_list:\n",
        "            prompt = f\"Q: {sentence}\\nA: This sentence is {sentiment_type}.\"\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Fine-tuning chatbot with input-output examples\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=100,\n",
        "                stop=[\"\\n\"]\n",
        "            )\n",
        "\n",
        "            predicted_sentiment = sentiment_analysis(response.choices[0].message.content)\n",
        "            actual_sentiment = categorize_sentiment(sentence)\n",
        "\n",
        "            if predicted_sentiment == actual_sentiment:\n",
        "                correct_predictions += 1\n",
        "            total_predictions += 1\n",
        "\n",
        "    accuracy = (correct_predictions / total_predictions) * 100\n",
        "    print(f\"Baseline Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Placeholder function for sentiment analysis (replace with actual implementation)\n",
        "def sentiment_analysis(text):\n",
        "    # Replace with your actual sentiment analysis logic\n",
        "    if \"positive\" in text.lower():\n",
        "        return 1  # Positive sentiment\n",
        "    elif \"negative\" in text.lower():\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/Sentences_AllAgree.txt\"\n",
        "    neutral_sentences, positive_sentences, negative_sentences = read_sentences_and_categorize(file_path)\n",
        "\n",
        "    sentences = {\n",
        "        \"neutral\": neutral_sentences,\n",
        "        \"positive\": positive_sentences,\n",
        "        \"negative\": negative_sentences\n",
        "    }\n",
        "\n",
        "    # Combine all sentences and fine-tune with the first 60 sentences\n",
        "    all_sentences = neutral_sentences + positive_sentences + negative_sentences\n",
        "    fine_tuned_responses = fine_tune_model(all_sentences[:60])  # Fine-tune only the first 60 sentences\n",
        "    evaluate_baseline_accuracy(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYWo5udE7gXM",
        "outputId": "f099c195-0479-421b-8959-0ff387b1afc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy: 93.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Then do 12-shot prompting, with 4 each for positive, negative, and neutral with the\n",
        "sentiment labels as input and then do an in-context testing on 50 to 60 news items for\n",
        "predicting the sentiment. This is probably easier done with prompt engineering.\n",
        "Technically, the accuracy should be better than zero-shot. But report the result as is.\n",
        "3. In both of the cases above use the data files AllAgree.txt or 75Agree.txt. For this\n",
        "assignment we will not use 66Agree or the 50Agree data files at all.\n"
      ],
      "metadata": {
        "id": "AdNVqqoEW20X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Step 1: Read Sentences from File and Categorize Sentiments\n",
        "def read_sentences_and_categorize(file_path):\n",
        "    neutral_sentences = []\n",
        "    positive_sentences = []\n",
        "    negative_sentences = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:\n",
        "        sentences = file.readlines()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentiment = categorize_sentiment(sentence)\n",
        "        if sentiment == 0:\n",
        "            neutral_sentences.append(sentence.strip())\n",
        "        elif sentiment == 1:\n",
        "            positive_sentences.append(sentence.strip())\n",
        "        elif sentiment == -1:\n",
        "            negative_sentences.append(sentence.strip())\n",
        "\n",
        "    return neutral_sentences, positive_sentences, negative_sentences\n",
        "\n",
        "# Placeholder function for sentiment categorization (replace with actual logic)\n",
        "def categorize_sentiment(sentence):\n",
        "    # Replace with your actual sentiment analysis logic\n",
        "    if \"good\" in sentence.lower() or \"positive keyword\" in sentence.lower():\n",
        "        return 1  # Positive sentiment\n",
        "    elif \"bad\" in sentence.lower() or \"negative keyword\" in sentence.lower():\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "# Step 2: Fine-tune the Model with 12-shot Prompts\n",
        "def fine_tune_model():\n",
        "    # Define prompts for 12-shot prompting\n",
        "    prompts = [\n",
        "        (\"Positive: This is a positive example.\", \"positive\"),\n",
        "        (\"Positive: Another positive example.\", \"positive\"),\n",
        "        (\"Positive: Yet another positive example.\", \"positive\"),\n",
        "        (\"Positive: One more positive example.\", \"positive\"),\n",
        "        (\"Negative: This is a negative example.\", \"negative\"),\n",
        "        (\"Negative: Another negative example.\", \"negative\"),\n",
        "        (\"Negative: Yet another negative example.\", \"negative\"),\n",
        "        (\"Negative: One more negative example.\", \"negative\"),\n",
        "        (\"Neutral: This is a neutral example.\", \"neutral\"),\n",
        "        (\"Neutral: Another neutral example.\", \"neutral\"),\n",
        "        (\"Neutral: Yet another neutral example.\", \"neutral\"),\n",
        "        (\"Neutral: One more neutral example.\", \"neutral\")\n",
        "    ]\n",
        "\n",
        "    fine_tuned_responses = []\n",
        "\n",
        "    for prompt, label in prompts:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Fine-tuning chatbot with 12-shot prompting\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Q: {prompt}\\nA: This sentence is {label}.\"}\n",
        "            ],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        fine_tuned_responses.append(response.choices[0].message.content)\n",
        "\n",
        "    return fine_tuned_responses\n",
        "\n",
        "# Step 3: In-Context Testing on News Items\n",
        "def test_on_news_items(news_items):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for news_item in news_items:\n",
        "        prompt = f\"Q: {news_item}\\nA:\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"In-context testing with news items\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=100,\n",
        "            stop=[\"\\n\"]\n",
        "        )\n",
        "\n",
        "        predicted_sentiment = sentiment_analysis(response.choices[0].message.content)\n",
        "        actual_sentiment = categorize_sentiment(news_item)\n",
        "\n",
        "        if predicted_sentiment == actual_sentiment:\n",
        "            correct_predictions += 1\n",
        "        total_predictions += 1\n",
        "\n",
        "    accuracy = (correct_predictions / total_predictions) * 100\n",
        "    print(f\"Accuracy on news items: {accuracy:.2f}%\")\n",
        "\n",
        "# Placeholder function for sentiment analysis (replace with actual implementation)\n",
        "def sentiment_analysis(text):\n",
        "    # Replace with your actual sentiment analysis logic\n",
        "    if \"positive\" in text.lower():\n",
        "        return 1  # Positive sentiment\n",
        "    elif \"negative\" in text.lower():\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/Sentences_AllAgree.txt\"\n",
        "    neutral_sentences, positive_sentences, negative_sentences = read_sentences_and_categorize(file_path)\n",
        "\n",
        "    # Combine all sentences for fine-tuning\n",
        "    all_sentences = neutral_sentences + positive_sentences + negative_sentences\n",
        "\n",
        "    # Fine-tune the model with 12-shot prompts\n",
        "    fine_tune_model()\n",
        "\n",
        "    # Test on news items (assuming news_items is a list of 50 to 60 news sentences)\n",
        "    news_items = [\n",
        "        \"This is a news headline about something positive happening.\",\n",
        "        \"Another news item describing a negative event that occurred recently.\",\n",
        "        \"A neutral news piece discussing recent developments in technology.\",\n",
        "        # Add more news sentences as needed\n",
        "    ]\n",
        "    test_on_news_items(news_items)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnaZ7ZV6DUkz",
        "outputId": "779f30c2-e276-4002-eabf-8a2da8db33c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on news items: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Then fine-tune an LLM with 450 from AllAgree and 75Agree datasets with 150 news\n",
        "items for each sentiment. Then use the fine-tuned model to predict a total of 500 (not\n",
        "including the dataset for training). Here, chose the news items randomly (they do not\n",
        "have to be distributed equally).\n"
      ],
      "metadata": {
        "id": "r9OXsQQBWz4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import random\n",
        "\n",
        "# Function to read and categorize sentences from datasets\n",
        "def read_and_categorize_datasets(file_paths):\n",
        "    neutral_sentences = []\n",
        "    positive_sentences = []\n",
        "    negative_sentences = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r', encoding='latin-1') as file:\n",
        "            sentences = file.readlines()\n",
        "\n",
        "        for sentence in sentences[:450]:  # Take first 450 sentences from each dataset\n",
        "            sentiment = categorize_sentiment(sentence)\n",
        "            if sentiment == 0:\n",
        "                neutral_sentences.append(sentence.strip())\n",
        "            elif sentiment == 1:\n",
        "                positive_sentences.append(sentence.strip())\n",
        "            elif sentiment == -1:\n",
        "                negative_sentences.append(sentence.strip())\n",
        "\n",
        "    return neutral_sentences, positive_sentences, negative_sentences\n",
        "\n",
        "# Placeholder function for sentiment categorization (replace with actual logic)\n",
        "def categorize_sentiment(sentence):\n",
        "    # Replace with your actual sentiment analysis logic\n",
        "    if \"good\" in sentence.lower() or \"positive keyword\" in sentence.lower():\n",
        "        return 1  # Positive sentiment\n",
        "    elif \"bad\" in sentence.lower() or \"negative keyword\" in sentence.lower():\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "# Function to fine-tune the language model\n",
        "def fine_tune_model(sentences):\n",
        "    fine_tuned_responses = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        prompt = f\"Input: {sentence}\\nOutput:\\n\"\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Fine-tuning chatbot with input-output examples\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        fine_tuned_responses.append(response.choices[0].message.content)  # Append each response content\n",
        "\n",
        "    return \"\\n\".join(fine_tuned_responses)  # Join all responses with newline and return as a single string\n",
        "\n",
        "\n",
        "# Function to evaluate model accuracy on news items\n",
        "def evaluate_on_news_items(news_items, model):\n",
        "    predictions = []\n",
        "\n",
        "    for news_item in news_items:\n",
        "        prompt = f\"Q: {news_item}\\nA:\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Predicting sentiment on news items\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=100,\n",
        "            stop=[\"\\n\"]\n",
        "        )\n",
        "\n",
        "        predicted_sentiment = sentiment_analysis(response.choices[0].message.content)\n",
        "        predictions.append(predicted_sentiment)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Placeholder function for sentiment analysis (replace with actual implementation)\n",
        "def sentiment_analysis(text):\n",
        "    # Replace with your actual sentiment analysis logic\n",
        "    if \"positive\" in text.lower():\n",
        "        return 1  # Positive sentiment\n",
        "    elif \"negative\" in text.lower():\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define file paths for datasets\n",
        "    all_agree_path = \"/content/Sentences_AllAgree.txt\"\n",
        "    agree_75_path = \"/content/Sentences_75Agree.txt\"\n",
        "\n",
        "    # Read and categorize sentences from datasets\n",
        "    all_agree_neutral, all_agree_positive, all_agree_negative = read_and_categorize_datasets([all_agree_path])\n",
        "    agree_75_neutral, agree_75_positive, agree_75_negative = read_and_categorize_datasets([agree_75_path])\n",
        "\n",
        "    # Combine datasets for fine-tuning\n",
        "    combined_neutral = all_agree_neutral + agree_75_neutral\n",
        "    combined_positive = all_agree_positive + agree_75_positive\n",
        "    combined_negative = all_agree_negative + agree_75_negative\n",
        "\n",
        "    # Randomly select 150 news items for each sentiment category for evaluation\n",
        "    random.seed(42)  # For reproducibility\n",
        "    random.shuffle(combined_neutral)\n",
        "    random.shuffle(combined_positive)\n",
        "    random.shuffle(combined_negative)\n",
        "\n",
        "    eval_neutral = combined_neutral[:150]\n",
        "    eval_positive = combined_positive[:150]\n",
        "    eval_negative = combined_negative[:150]\n",
        "\n",
        "    # Fine-tune the model with the combined dataset\n",
        "    fine_tune_sentences = combined_neutral + combined_positive + combined_negative\n",
        "    fine_tune_model(fine_tune_sentences)\n",
        "\n",
        "    # Select 500 additional news items for prediction\n",
        "    # (These should not overlap with the evaluation set)\n",
        "    additional_news_items = [...]  # Define your additional news items here\n",
        "\n",
        "    # Use the fine-tuned model to predict sentiment on additional news items\n",
        "    predictions = evaluate_on_news_items(additional_news_items, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Print or further process the predictions as needed\n",
        "    print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJdsZyUkQpnl",
        "outputId": "1b9fb81d-e7d9-412a-c1b0-8cc21f25384a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Tabulate the results with a confusion matrix using True positive, True negative, False\n",
        "positive, and False negative for the fine-tuning experiment. Read Wikipedia or other\n",
        "sources for learning about the confusion matrix."
      ],
      "metadata": {
        "id": "IOe-DWVlWh4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to read sentences from file and categorize sentiments for the first 25 sentences\n",
        "def read_and_categorize_first_25(file_path):\n",
        "    neutral_tp, neutral_fp, neutral_fn, neutral_tn = 0, 0, 0, 0\n",
        "    positive_tp, positive_fp, positive_fn, positive_tn = 0, 0, 0, 0\n",
        "    negative_tp, negative_fp, negative_fn, negative_tn = 0, 0, 0, 0\n",
        "\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:\n",
        "        sentences = file.readlines()  # Read only the first 25 sentences\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentiment = categorize_sentiment(sentence)\n",
        "        actual_sentiment = categorize_actual_sentiment(sentence)\n",
        "\n",
        "        if actual_sentiment == 0:  # Neutral\n",
        "            if sentiment == 1:  # Predicted positive\n",
        "                neutral_fp += 1\n",
        "            elif sentiment == -1:  # Predicted negative\n",
        "                neutral_fn += 1\n",
        "            elif sentiment == 0:  # Predicted neutral\n",
        "                neutral_tn += 1\n",
        "        elif actual_sentiment == 1:  # Positive\n",
        "            if sentiment == 1:  # Predicted positive\n",
        "                positive_tp += 1\n",
        "            elif sentiment == -1:  # Predicted negative\n",
        "                positive_fp += 1\n",
        "            elif sentiment == 0:  # Predicted neutral\n",
        "                positive_fn += 1\n",
        "        elif actual_sentiment == -1:  # Negative\n",
        "            if sentiment == 1:  # Predicted positive\n",
        "                negative_fp += 1\n",
        "            elif sentiment == -1:  # Predicted negative\n",
        "                negative_tp += 1\n",
        "            elif sentiment == 0:  # Predicted neutral\n",
        "                negative_fn += 1\n",
        "\n",
        "    return (neutral_tp, neutral_fp, neutral_fn, neutral_tn,\n",
        "            positive_tp, positive_fp, positive_fn, positive_tn,\n",
        "            negative_tp, negative_fp, negative_fn, negative_tn)\n",
        "\n",
        "# Placeholder function for sentiment categorization (replace with actual logic)\n",
        "def categorize_sentiment(sentence):\n",
        "    # Replace with your actual sentiment analysis logic\n",
        "    if \"good\" in sentence.lower() or \"positive keyword\" in sentence.lower():\n",
        "        return 1  # Positive sentiment\n",
        "    elif \"bad\" in sentence.lower() or \"negative keyword\" in sentence.lower():\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "# Placeholder function for actual sentiment categorization (replace with actual logic)\n",
        "def categorize_actual_sentiment(sentence):\n",
        "    # Replace with your actual logic to categorize actual sentiment from text\n",
        "    if \"neutral\" in sentence.lower():\n",
        "        return 0\n",
        "    elif \"positive\" in sentence.lower():\n",
        "        return 1\n",
        "    elif \"negative\" in sentence.lower():\n",
        "        return -1\n",
        "    else:\n",
        "        return 0  # Default to neutral if sentiment not clearly identifiable\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/Sentences_AllAgree.txt\"\n",
        "    (neutral_tp, neutral_fp, neutral_fn, neutral_tn,\n",
        "     positive_tp, positive_fp, positive_fn, positive_tn,\n",
        "     negative_tp, negative_fp, negative_fn, negative_tn) = read_and_categorize_first_25(file_path)\n",
        "\n",
        "    # Construct confusion matrices for the first 25 sentences\n",
        "    confusion_matrix_neutral = [\n",
        "        [neutral_tp, neutral_fp],\n",
        "        [neutral_fn, neutral_tn]\n",
        "    ]\n",
        "    confusion_matrix_positive = [\n",
        "        [positive_tp, positive_fp],\n",
        "        [positive_fn, positive_tn]\n",
        "    ]\n",
        "    confusion_matrix_negative = [\n",
        "        [negative_tp, negative_fp],\n",
        "        [negative_fn, negative_tn]\n",
        "    ]\n",
        "\n",
        "    # Create a DataFrame for confusion matrices\n",
        "    data = {\n",
        "        \"Sentiment\": [\"Neutral\", \"Positive\", \"Negative\"],\n",
        "        \"True Positive\": [neutral_tp, positive_tp, negative_tp],\n",
        "        \"False Positive\": [neutral_fp, positive_fp, negative_fp],\n",
        "        \"False Negative\": [neutral_fn, positive_fn, negative_fn],\n",
        "        \"True Negative\": [neutral_tn, positive_tn, negative_tn]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Print DataFrame (optional)\n",
        "    print(\"Confusion Matrix Data:\")\n",
        "    print(df)\n",
        "\n",
        "    # Export DataFrame to Excel\n",
        "    excel_file = \"/content/ConfusionMatrices.xlsx\"\n",
        "    df.to_excel(excel_file, index=False)\n",
        "    print(f\"Confusion matrices exported to '{excel_file}' successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyHoXwDeP1Dj",
        "outputId": "573072bf-27d9-4d32-e8c0-d7a338bba2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix Data:\n",
            "  Sentiment  True Positive  False Positive  False Negative  True Negative\n",
            "0   Neutral              0              11               0           1381\n",
            "1  Positive              5               0             564              0\n",
            "2  Negative              1               1             301              0\n",
            "Confusion matrices exported to '/content/ConfusionMatrices.xlsx' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Lastly, go to finance.yahoo.com or other financial news websites, get 10 news items\n",
        "(perhaps longer than a sentence) on different companies and test them with the fine-\n",
        "tuned model. Then do a human level accuracy check and report the results.\n"
      ],
      "metadata": {
        "id": "Rt77MdqoWbUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Function to predict sentiment using the fine-tuned model\n",
        "def predict_sentiment(news_items, model):\n",
        "    predictions = []\n",
        "\n",
        "    for news_item in news_items:\n",
        "        prompt = f\"Q: {news_item}\\nA:\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Predicting sentiment on news items\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=100,\n",
        "            stop=[\"\\n\"]\n",
        "        )\n",
        "\n",
        "        predicted_sentiment = sentiment_analysis(response.choices[0].message.content)\n",
        "        predictions.append(predicted_sentiment)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Placeholder function for sentiment analysis (replace with actual implementation)\n",
        "def sentiment_analysis(text):\n",
        "    # Replace with your actual sentiment analysis logic\n",
        "    if \"positive\" in text.lower():\n",
        "        return \"Positive\"\n",
        "    elif \"negative\" in text.lower():\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define your 10 news items from financial news websites here\n",
        "    news_items = [\n",
        "       \"Tesla's strong quarterly earnings surpassed expectations, driving its stock price to new heights in the market.\",\n",
        "       \"Amazon committed to substantial investments in green energy initiatives, reinforcing its sustainability goals and earning positive market reception.\",\n",
        "       \"Apple overcame supply chain challenges with the successful launch of a groundbreaking new product, bolstering its innovation reputation amidst adversity.\",\n",
        "       \"Microsoft achieved record-breaking revenue in its cloud computing sector, demonstrating robust growth and market dominance in cloud services.\",\n",
        "       \"Google faced significant antitrust scrutiny across major global markets, impacting its regulatory landscape and market perception.\",\n",
        "       \"Meta, Facebook's parent company, saw promising growth in the virtual reality sector, highlighting potential new revenue streams amid evolving tech trends.\",\n",
        "       \"Netflix experienced a slowdown in subscriber growth, leading to a decline in its stock performance and investor confidence.\",\n",
        "       \"Alibaba encountered regulatory crackdowns in China, resulting in a notable drop in its stock price and uncertainty about its future operations.\",\n",
        "       \"JP Morgan Chase announced ambitious plans to expand into digital banking, signaling a strategic shift towards innovative financial services.\",\n",
        "       \"Intel unveiled its next-generation processors promising improved performance, positioning itself competitively in the semiconductor market with cutting-edge technology.\"\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Predict sentiment using the fine-tuned model\n",
        "    predictions = predict_sentiment(news_items, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Human-level accuracy check (manually assess each news item)\n",
        "    human_assessment = [\n",
        "        \"Positive\",  # Tesla\n",
        "        \"Positive\",  # Amazon\n",
        "        \"Neutral\",   # Apple\n",
        "        \"Positive\",  # Microsoft\n",
        "        \"Negative\",  # Google\n",
        "        \"Positive\",  # Meta\n",
        "        \"Negative\",  # Netflix\n",
        "        \"Negative\",  # Alibaba\n",
        "        \"Positive\",  # JP Morgan Chase\n",
        "        \"Positive\"   # Intel\n",
        "    ]\n",
        "\n",
        "    # Compare predictions with human assessment\n",
        "    correct_predictions = sum(1 for pred, human in zip(predictions, human_assessment) if pred == human)\n",
        "    total_news_items = len(news_items)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (correct_predictions / total_news_items) * 100\n",
        "\n",
        "    # Report results\n",
        "    print(\"\\nNews Items:\")\n",
        "    for idx, news_item in enumerate(news_items, start=1):\n",
        "        print(f\"{idx}. {news_item}\")\n",
        "\n",
        "    print(\"\\nPredicted Sentiments:\")\n",
        "    for idx, pred in enumerate(predictions, start=1):\n",
        "        print(f\"{idx}. {pred}\")\n",
        "\n",
        "    print(\"\\nHuman Assessment:\")\n",
        "    for idx, human in enumerate(human_assessment, start=1):\n",
        "        print(f\"{idx}. {human}\")\n",
        "\n",
        "    print(f\"\\nCorrect Predictions: {correct_predictions} out of {total_news_items}\")\n",
        "    print(f\"Human-Level Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wewa3cg8V5zb",
        "outputId": "f4d296ae-3815-4420-91c7-33200ad43342",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "News Items:\n",
            "1. Tesla's strong quarterly earnings surpassed expectations, driving its stock price to new heights in the market.\n",
            "2. Amazon committed to substantial investments in green energy initiatives, reinforcing its sustainability goals and earning positive market reception.\n",
            "3. Apple overcame supply chain challenges with the successful launch of a groundbreaking new product, bolstering its innovation reputation amidst adversity.\n",
            "4. Microsoft achieved record-breaking revenue in its cloud computing sector, demonstrating robust growth and market dominance in cloud services.\n",
            "5. Google faced significant antitrust scrutiny across major global markets, impacting its regulatory landscape and market perception.\n",
            "6. Meta, Facebook's parent company, saw promising growth in the virtual reality sector, highlighting potential new revenue streams amid evolving tech trends.\n",
            "7. Netflix experienced a slowdown in subscriber growth, leading to a decline in its stock performance and investor confidence.\n",
            "8. Alibaba encountered regulatory crackdowns in China, resulting in a notable drop in its stock price and uncertainty about its future operations.\n",
            "9. JP Morgan Chase announced ambitious plans to expand into digital banking, signaling a strategic shift towards innovative financial services.\n",
            "10. Intel unveiled its next-generation processors promising improved performance, positioning itself competitively in the semiconductor market with cutting-edge technology.\n",
            "\n",
            "Predicted Sentiments:\n",
            "1. Positive\n",
            "2. Positive\n",
            "3. Positive\n",
            "4. Positive\n",
            "5. Negative\n",
            "6. Positive\n",
            "7. Negative\n",
            "8. Negative\n",
            "9. Positive\n",
            "10. Positive\n",
            "\n",
            "Human Assessment:\n",
            "1. Positive\n",
            "2. Positive\n",
            "3. Neutral\n",
            "4. Positive\n",
            "5. Negative\n",
            "6. Positive\n",
            "7. Negative\n",
            "8. Negative\n",
            "9. Positive\n",
            "10. Positive\n",
            "\n",
            "Correct Predictions: 9 out of 10\n",
            "Human-Level Accuracy: 90.00%\n"
          ]
        }
      ]
    }
  ]
}